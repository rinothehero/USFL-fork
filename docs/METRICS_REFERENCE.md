# Metrics Reference

Complete reference for every metric computed across the three frameworks (SFL, GAS, MultiSFL).

**Last updated:** 2026-02-08
**Scope:** All computed, logged, and saved metrics

---

## Table of Contents

1. [Overview & Metric Categories](#1-overview--metric-categories)
2. [G Measurement (Oracle Gradient Distance)](#2-g-measurement-oracle-gradient-distance)
3. [Drift Measurement (SCAFFOLD-style)](#3-drift-measurement-scaffold-style)
4. [Update Alignment (A_cos + M_norm)](#4-update-alignment-a_cos--m_norm)
5. [Training Metrics (Accuracy & Loss)](#5-training-metrics-accuracy--loss)
6. [GAS-Specific Metrics](#6-gas-specific-metrics)
7. [USFL-Specific Metrics](#7-usfl-specific-metrics)
8. [MultiSFL-Specific Metrics](#8-multisfl-specific-metrics)
9. [Configuration Reference](#9-configuration-reference)
10. [Result JSON Schema](#10-result-json-schema)

---

## 1. Overview & Metric Categories

The metrics fall into four conceptual layers:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 4: Framework-Specific Metrics                         â”‚
â”‚  V-value (GAS), Freshness Score (USFL), FGN (MultiSFL)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 3: Update Alignment   (A_cos, M_norm)                 â”‚
â”‚  Scale-invariant, comparable across all frameworks           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 2: Drift Measurement  (G_drift, G_end, D_dir, D_rel) â”‚
â”‚  Per-round trajectory tracking during local optimization     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 1: G Measurement      (G, G_rel, D_cosine)            â”‚
â”‚  Oracle-based gradient quality (computed on diagnostic rounds)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Category | Frequency | Frameworks | Key Question Answered |
|----------|-----------|------------|----------------------|
| G Measurement | Every N rounds | All | "How far are training gradients from the true gradient?" |
| Drift Measurement | Every round | All | "How much do local updates deviate from the global model during a round?" |
| Update Alignment | Every round | All | "Do clients agree on which direction to update?" |
| Training Metrics | Every round | All | "How well is the model learning?" |

---

## 2. G Measurement (Oracle Gradient Distance)

Compares actual training gradients against the **oracle gradient** (the gradient you would get by training on the *entire* dataset at once). Measures how well the federated/split training approximates centralized training.

### 2.1 Oracle Gradient (g\*)

The oracle is the reference "perfect" gradient. Computed as:

```
g* = (1 / |D|) * Î£_{(x,y) âˆˆ D} âˆ‡L(x, y)
```

Where `D` is the full training dataset and `L` is cross-entropy loss with `reduction='sum'`.

**Implementation details (shared across all frameworks):**
- Forward pass through the **entire** training dataset in batches
- Gradients accumulated with `reduction='sum'`, then divided by `total_samples`
- Model kept in `train()` mode (same conditions as actual training)
- BatchNorm statistics backed up before oracle computation and restored after
- Split into client-side oracle (`g*_client`) and server-side oracle (`g*_server`)

**Oracle modes:**

| Mode | Config Value | SFL/GAS | MultiSFL | Description |
|------|-------------|---------|----------|-------------|
| Global | `"global"` | `"strict"` | `"master"` | Single oracle using the current aggregated global model |
| Individual | `"individual"` | `"realistic"` | `"branch"` | Per-model oracle (per-client in SFL/GAS, per-branch in MultiSFL) |

### 2.2 Core G Metrics (the triplet)

Every G measurement produces three values. These are computed identically in all frameworks via `compute_g_metrics()`:

#### G (Squared L2 Distance)

```
G = â€–gÌƒ - g*â€–Â² = (gÌƒ - g*)áµ€(gÌƒ - g*)
```

| | |
|---|---|
| **What it measures** | Absolute squared distance between the training gradient (`gÌƒ`) and the oracle gradient (`g*`) |
| **Range** | [0, +âˆž) â€” lower is better |
| **Unit** | Squared gradient norm (scale-dependent) |
| **Limitation** | Sensitive to learning rate, batch size, and model scale. Not directly comparable across different hyperparameter settings |

#### G_rel (Relative Distance)

```
G_rel = G / (â€–g*â€–Â² + Îµ)       where Îµ = 1e-8
```

| | |
|---|---|
| **What it measures** | G normalized by the oracle's magnitude. "How large is the error relative to the signal?" |
| **Range** | [0, +âˆž) â€” lower is better |
| **Interpretation** | G_rel = 0.5 means the error is half the oracle magnitude. G_rel > 1 means the error exceeds the signal |
| **Advantage** | Comparable across models of different sizes and learning rates |

#### D_cosine (Cosine Distance)

```
D_cosine = 1 - cos(gÌƒ, g*) = 1 - (gÌƒ Â· g*) / (â€–gÌƒâ€– Â· â€–g*â€–)
```

Clamped to [0, 2] for numerical stability.

| | |
|---|---|
| **What it measures** | Directional disagreement between training and oracle gradients |
| **Range** | [0, 2] â€” 0 = same direction, 1 = orthogonal, 2 = opposite |
| **Advantage** | Purely directional â€” immune to gradient magnitude/scale |

### 2.3 Perspectives (Client, Server, Split Layer)

Each G measurement is computed from three perspectives corresponding to the split learning architecture:

```
Input â†’ [Client Model] â†’ activations â†’ [Server Model] â†’ loss
         gÌƒ_client              gÌƒ_split         gÌƒ_server
```

| Perspective | What gradient is measured | Granularity |
|-------------|-------------------------|-------------|
| **Client G** | `âˆ‡_client_params L` â€” gradients of the bottom model | Per-client (individual), then averaged |
| **Server G** | `âˆ‡_server_params L` â€” gradients of the top model | Per-batch or per-branch |
| **Split G** | `âˆ‚L/âˆ‚activations` â€” gradient at the cut point | Averaged across clients |

### 2.4 Measurement Modes

How training gradients (`gÌƒ`) are collected during a diagnostic round:

| Mode | Config | Description | Use Case |
|------|--------|-------------|----------|
| `single` | `g_measurement_mode: "single"` | First batch per client only | Fast, approximate |
| `k_batch` | `g_measurement_mode: "k_batch"` | First K batches, weighted average | Balanced accuracy/speed |
| `accumulated` | `g_measurement_mode: "accumulated"` | All batches in the round, weighted average | Most accurate, slowest |

Weighted averaging formula (for k_batch and accumulated):
```
gÌƒ_avg = (Î£_b  gÌƒ_b Ã— batch_size_b) / (Î£_b  batch_size_b)
```

### 2.5 Variance G (Optional)

When `use_variance_g = true`, computes a weighted-variance decomposition:

```
V_c = Î£_i (w_i / W) Ã— â€–gÌƒ_i - g*â€–Â²
```

Where `w_i = batch_size_i` and `W = Î£ w_i`.

| Metric | Formula | Purpose |
|--------|---------|---------|
| `variance_client_g` | V_c | Weighted sum of per-client G values (accounts for data volume) |
| `variance_client_g_rel` | V_c / â€–g\*â€–Â² | Scale-invariant variant |
| `variance_server_g` | Same formula over server gradients | Server-side variance |
| `variance_server_g_rel` | V_s / â€–g\*â€–Â² | Scale-invariant server variant |

**Difference from mean G:** Mean G treats each client equally (1/N weighting). Variance G weights by data contribution (batch_size_i / total), giving more influence to clients that process more data.

### 2.6 Source Locations

| Framework | File | Class/Function |
|-----------|------|----------------|
| SFL | `sfl_framework-.../server/utils/g_measurement.py` | `GMeasurementSystem` (line ~764) |
| GAS | `GAS_implementation/utils/g_measurement.py` | `GMeasurementManager` (line ~496) |
| MultiSFL | `multisfl_implementation/multisfl/g_measurement.py` | `GMeasurementSystem` (line ~440) |

---

## 3. Drift Measurement (SCAFFOLD-style)

Tracks how much local model parameters deviate from the round-start global model during a training round. Inspired by the SCAFFOLD paper's client drift analysis.

### 3.1 Core Concepts

At the start of each round, the global model parameters are `x^{t,0}`. During local training, each client takes B optimizer steps, producing `x^{t,1}, x^{t,2}, ..., x^{t,B}`. Drift metrics capture how this trajectory deviates.

```
x^{t,0} â”€â”€step 1â”€â”€> x^{t,1} â”€â”€step 2â”€â”€> x^{t,2} â”€â”€...â”€â”€> x^{t,B}
  â”‚                    â”‚                    â”‚                  â”‚
  â””â”€â”€â”€ driftâ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚                  â”‚
  â””â”€â”€â”€ driftâ‚‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
  â””â”€â”€â”€ drift_B (endpoint) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Only **trainable parameters** are included (BatchNorm running statistics are excluded).

### 3.2 Per-Client State

For each participating client `n` in round `t`:

| Symbol | Name | Formula | Description |
|--------|------|---------|-------------|
| S_n | `trajectory_sum` | `Î£_{b=1}^{B_n} â€–x_n^{t,b} - x_n^{t,0}â€–Â²` | Accumulated squared distance from round start after each optimizer step |
| B_n | `batch_steps` | Step counter | Number of optimizer steps taken |
| E_n | `endpoint_drift` | `â€–x_n^{t,B_n} - x_n^{t,0}â€–Â²` | Final squared distance from round start |

Where `â€–Â·â€–Â²` is the sum of squared differences across all trainable parameters:
```
â€–a - bâ€–Â² = Î£_{param âˆˆ trainable} Î£_{element} (a[param][element] - b[param][element])Â²
```

### 3.3 Client Drift Metrics (aggregated over clients)

#### G_drift_client (Average Trajectory Drift Energy)

```
G_drift_client = (1 / |P_t|) Ã— Î£_n (S_n / B_n)
```

| | |
|---|---|
| **What it measures** | Average per-step drift energy across participating clients |
| **Interpretation** | Higher = clients are drifting further from the global model during each step |
| **Use case** | Primary drift indicator. High G_drift_client suggests local training is pushing clients away from consensus |

#### G_drift_client_stepweighted (Step-Weighted Variant)

```
G_drift_client_stepweighted = Î£S / Î£B = (Î£_n S_n) / (Î£_n B_n)
```

| | |
|---|---|
| **Difference from G_drift_client** | Avoids "1 client = 1 vote" distortion when clients have different step counts |
| **Example** | If client A takes 10 steps and client B takes 1 step, G_drift_client weights them equally, but stepweighted gives client A 10x more influence |

#### G_end_client (Average Endpoint Drift)

```
G_end_client = (1 / |P_t|) Ã— Î£_n E_n
```

| | |
|---|---|
| **What it measures** | Average squared distance between each client's final model and the round-start global model |
| **Difference from G_drift** | G_drift captures the *journey* (trajectory), G_end captures only the *destination* |

#### G_end_client_weighted (Aggregation-Weight-Weighted Endpoint)

```
G_end_client_weighted = Î£_i (w_i / W) Ã— E_i
```

Where `w_i` are the FedAvg aggregation weights (proportional to dataset size). In GAS, aggregation is uniform, so this equals `G_end_client`.

### 3.4 Update Disagreement Metrics

These use a **variance decomposition identity** to measure how much individual client updates disagree with each other.

#### D_dir (Directional Disagreement)

```
D_dir = E_w[â€–Î”_iâ€–Â²] - â€–E_w[Î”_i]â€–Â²
      = G_end_client_weighted - delta_client_norm_sq
```

Where `Î”_i = x_i^{t,B} - x^{t,0}` is client i's update vector.

| | |
|---|---|
| **Mathematical identity** | This is exactly `Var_w(Î”_i)` â€” the weighted variance of client updates |
| **Interpretation** | D_dir = 0 means all clients agree perfectly. Higher = more disagreement |
| **Why "directional"** | It captures disagreement in both magnitude AND direction |

#### D_rel (Relative Disagreement)

```
D_rel = D_dir / (â€–Î”_globalâ€–Â² + Îµ)
```

| | |
|---|---|
| **What it measures** | Disagreement normalized by the aggregated update magnitude |
| **Interpretation** | D_rel = 1 means client disagreement is as large as the global update itself. D_rel >> 1 signals that clients are pulling in very different directions |

### 3.5 Server Drift Metrics

Same formulas as client drift but applied to the server model (top portion of the split):

| Metric | Formula |
|--------|---------|
| `G_drift_server` | `S_server / B_server` |
| `G_end_server` | `E_server = â€–x_s^{t,B} - x_s^{t,0}â€–Â²` |
| `delta_server_norm_sq` | `â€–x_s^{t+1,0} - x_s^{t,0}â€–Â²` (aggregated server update magnitude) |
| `G_drift_norm_server` | `G_drift_server / (delta_server_norm_sq + Îµ)` |

### 3.6 Global Update Magnitude

```
delta_client_norm_sq = â€–x_c^{t+1,0} - x_c^{t,0}â€–Â²
```

| | |
|---|---|
| **What it measures** | How much the aggregated global client model changed this round |
| **Purpose** | Used as denominator in normalized metrics (G_drift_norm, D_rel) |
| **Note** | This is the **aggregated** update â€” the weighted average of all client updates â€” not individual client drift |

### 3.7 Normalized Drift

```
G_drift_norm_client = G_drift_client / (delta_client_norm_sq + Îµ)
```

| | |
|---|---|
| **Purpose** | Prevents the "update suppression" criticism: if the model barely moves (small delta), raw drift might look small even though it's proportionally large |
| **Adaptive Îµ** | After 10 rounds, Îµ is set to `1e-3 Ã— median(delta_norms of first 10 rounds)`. This prevents division-by-near-zero instability in early training |

### 3.8 Combined Metrics

| Metric | Formula | Purpose |
|--------|---------|---------|
| `G_drift_total` | `G_drift_client + G_drift_server` | Total system drift (both halves of the split model) |
| `G_end_total` | `G_end_client + G_end_server` | Total endpoint drift |

### 3.9 Legacy Aliases

For backward compatibility, the result JSON also includes:

| Legacy Name | Actual Metric |
|-------------|---------------|
| `G_drift` | `G_drift_client` |
| `G_end` | `G_end_client` |
| `G_drift_norm` | `G_drift_norm_client` |
| `delta_global_norm_sq` | `delta_client_norm_sq` |
| `num_clients` | `num_clients` (GAS/SFL) or `num_branches` (MultiSFL) |

### 3.10 Source Locations

| Framework | File | Class |
|-----------|------|-------|
| SFL | `sfl_framework-.../server/utils/drift_measurement.py` | `DriftMeasurementTracker` |
| GAS | `GAS_implementation/utils/drift_measurement.py` | `DriftMeasurementTracker` |
| MultiSFL | `multisfl_implementation/multisfl/drift_measurement.py` | `MultiSFLDriftTracker` |

### 3.11 MultiSFL Drift Differences

In MultiSFL, drift is measured **per-branch** (not per-client):
- Each branch trains a copy of the model
- `S_b`, `B_b`, `E_b` are tracked per branch server
- `G_drift_client` averages over branches, not individual clients
- **Both** client-side and server-side `A_cos` are computed (unique to MultiSFL)

---

## 4. Update Alignment (A_cos + M_norm)

Scale-invariant metrics that measure how aligned client (or branch) updates are in their optimization direction. Designed to be **directly comparable across all frameworks** regardless of learning rate, batch size, or step count.

### 4.1 A_cos (Cosine Alignment)

```
A_cos = (Î£_{i<j} w_ij Ã— cos(Î”_i, Î”_j)) / (Î£_{i<j} w_ij)
```

Where:
- `Î”_i = flatten(Î¸_end_i - Î¸_start)` â€” client i's flattened parameter update vector
- `cos(a, b) = (a Â· b) / (â€–aâ€– Ã— â€–bâ€–)` â€” cosine similarity
- `w_ij = w_i Ã— w_j` â€” pair weight (product of individual aggregation weights)
- Only trainable parameters are included (BatchNorm buffers excluded)

| | |
|---|---|
| **Range** | [-1, 1] (NaN if fewer than 2 valid clients) |
| **A_cos = 1** | All clients update in exactly the same direction (perfect alignment) |
| **A_cos â‰ˆ 0** | Client updates are roughly orthogonal (no systematic agreement) |
| **A_cos < 0** | Clients actively disagree (updates point in opposing directions) |
| **Threshold** | Clients with `â€–Î”_iâ€– â‰¤ Ï„` (default Ï„ = 1e-7) are excluded from A_cos |
| **Key advantage** | Immune to LR, batch size, and step count differences across frameworks |

**Intuition:** Think of each client's update as an arrow in high-dimensional space. A_cos measures whether these arrows point in the same direction, regardless of their length.

### 4.2 M_norm (Mean Update Magnitude)

```
M_norm = (Î£_i w_i Ã— â€–Î”_iâ€–) / (Î£_i w_i)
```

| | |
|---|---|
| **Range** | [0, +âˆž) |
| **What it measures** | Average L2 norm of client update vectors |
| **Purpose** | Complements A_cos â€” tells you HOW MUCH clients are moving, while A_cos tells you WHETHER they agree on direction |
| **Includes all clients** | Unlike A_cos, clients below the Ï„ threshold are still included |

### 4.3 Relationship Between A_cos and D_rel

Both measure client agreement but from different angles:

| Aspect | A_cos | D_rel |
|--------|-------|-------|
| **What it compares** | Pairwise client directions | Clients vs. aggregated mean |
| **Scale invariance** | Yes (cosine-based) | No (uses squared norms) |
| **Sensitivity** | Direction only | Direction + magnitude |
| **Cross-framework comparable** | Yes | Only within same hyperparameters |

### 4.4 MultiSFL Dual A_cos

MultiSFL computes **separate** alignment metrics for client and server sides:

| Metric | What it measures |
|--------|-----------------|
| `A_cos_client` | Alignment of branch client model updates |
| `A_cos_server` | Alignment of branch server model updates |
| `M_norm_client` | Mean branch client update magnitude |
| `M_norm_server` | Mean branch server update magnitude |

This is unique to MultiSFL because it has per-branch server models. SFL and GAS only compute a single A_cos over client updates.

### 4.5 Source Location

| File | Line |
|------|------|
| `shared/update_alignment.py` | `compute_update_alignment()` at line 82 |
| `shared/update_alignment.py` | `flatten_delta()` at line 34 |

---

## 5. Training Metrics (Accuracy & Loss)

### 5.1 Test Accuracy

```
accuracy = correct / total
```

Where `correct = Î£ ðŸ™[argmax(model(x)) == y]` over the test set.

| Framework | Location | Frequency |
|-----------|----------|-----------|
| SFL | `server/modules/model/flexible_resnet.py` + stage organizer `_post_round` | Every round |
| GAS | `GAS_main.py:1286-1296` | Every `Accu_Test_Frequency` rounds (default: 1) |
| MultiSFL | `multisfl/trainer.py:147-195` | Every round |

**MultiSFL note:** Accuracy is evaluated using the **master model** (averaged across all branches), not individual branch models.

### 5.2 Training Loss

```
loss = CrossEntropyLoss(server_model(activations), labels)
```

| Framework | Granularity | Saved? |
|-----------|-------------|--------|
| SFL | Averaged per round: `epoch_loss = Î£ loss / num_iterations` | Logged to TrainingTracker |
| GAS | Per-step (client + server) | Not saved to results |
| MultiSFL | Per-step per-branch | Not saved to results |

### 5.3 NLP Metrics (SFL only)

For NLP tasks in the SFL framework, additional metrics are computed during in-round evaluation:

| Task | Metrics | Location |
|------|---------|----------|
| MRPC, QQP | F1 score + accuracy | `sfl_stage_organizer.py:581-585` |
| CoLA | Matthews correlation coefficient | `sfl_stage_organizer.py:587-589` |
| STS-B | Pearson + Spearman correlation | `sfl_stage_organizer.py:591-593` |
| Others | Accuracy | Default |

---

## 6. GAS-Specific Metrics

### 6.1 V-Value (Gradient Dissimilarity)

Measures how well the cached split-layer activations represent the true data distribution.

```
g_real    = (1/M) Ã— Î£_{m=1}^{M} âˆ‡L(server_model, test_batch_m)
g_sampled = âˆ‡L(server_model, concat_features)

V = (1/|params|) Ã— Î£_p â€–g_sampled_p - g_real_pâ€–Â²
```

Where `concat_features` are the split-layer activations from participating clients in the most recent round, and `M` = number of test minibatches (default: 10).

| | |
|---|---|
| **Range** | [0, +âˆž) â€” lower is better |
| **Interpretation** | Measures how different the server gradient from cached activations is compared to the "true" gradient on test data |
| **Purpose** | Evaluates the quality of GAS's feature sampling/generation mechanism |
| **When computed** | Every `V_Test_Frequency` rounds (default: 1), only if `V_Test=True` |
| **Location** | `utils/utils.py:57-110` (`calculate_v_value`) |

### 6.2 Split-Layer G (Split G)

GAS computes an additional G metric at the split point (activation gradients):

```
gÌƒ_split_avg = mean(gÌƒ_split_i for each participating client i)
split_G = â€–gÌƒ_split_avg - g*_splitâ€–Â²
```

| | |
|---|---|
| **What it measures** | How far the average activation-gradient at the cut point is from the oracle |
| **Location** | `GAS_main.py:734-741` |
| **Note** | GAS captures only the first batch for split-layer oracle (memory optimization), while SFL accumulates across all batches |

### 6.3 Logit Local Adjustment

Per-client logit adjustment for label imbalance:

```
label_freq[k] = count of label k in client's data
p[k] = label_freq[k] / Î£ label_freq
adjustment[k] = log(p[k]^Ï„ + 1e-12)     where Ï„ = 1 (default)
```

Applied during training as: `loss = CE(output + adjustment, labels)`

| | |
|---|---|
| **Purpose** | Compensates for label frequency imbalance in Non-IID settings |
| **Location** | `utils/utils.py:221-239` (`compute_local_adjustment`) |
| **Saved?** | No â€” computed once at initialization, used internally |

### 6.4 Activation Statistics (IncrementalStats)

Running statistics of split-layer activations for synthetic feature generation:

```
decay = old_weight / (old_weight + new_weight)
new_mean = decay Ã— old_mean + (1 - decay) Ã— batch_mean

# ResNet (diagonal variance):
new_var = decay Ã— (old_var + (new_mean - old_mean)Â²)
        + (1 - decay) Ã— (batch_var + (new_mean - batch_mean)Â²) + 1e-5

# AlexNet (full covariance):
new_cov = decay Ã— (old_cov + outer(diff_old))
        + (1 - decay) Ã— (batch_cov + outer(diff_new)) + 1e-5 Ã— I
```

| | |
|---|---|
| **Granularity** | Per-label, global |
| **Purpose** | Enables generation of synthetic activations for replay when real clients are unavailable |
| **Location** | `GAS_main.py:330-403` (`IncrementalStats`) |
| **Saved?** | No â€” ephemeral training state |

### 6.5 Communication Time Simulation

Simulates wireless channel latency for heterogeneous clients:

```
path_loss = 128.1 + 37.6 Ã— log10(distance_km)
h = 10^(-path_loss / 10)
rate = W Ã— log2(1 + (P Ã— h) / (W Ã— Nâ‚€))

model_process_time     = FLOPs / computing_capacity
transmit_activation_time = (activation_bits Ã— batch_size) / rate
transmit_model_time    = model_bits / rate
```

Where W = 10 MHz, P = 0.2 W, Nâ‚€ = 3.981e-21 W/Hz.

| | |
|---|---|
| **Saved as** | `time_record[round] = max(local_models_time)` |
| **Purpose** | Simulates asynchronous SFL communication overhead |
| **Location** | `GAS_main.py:227-327` (Client class) |
| **Enabled when** | `WRTT=True` |

---

## 7. USFL-Specific Metrics

### 7.1 KL Divergence (Class Imbalance)

```
KL_scaled = KL(empirical â€– uniform) / log(C)
         = (Î£_k p_k Ã— log(p_k Ã— C)) / log(C)
```

Where `p_k` is the empirical class proportion and `C` is the number of classes.

| | |
|---|---|
| **Range** | [0, 1] â€” 0 = perfectly balanced, 1 = maximally imbalanced |
| **Purpose** | Measures class imbalance of a batch or client's data |
| **Location** | `usfl_stage_organizer.py:291-316` |
| **Saved?** | No â€” used internally for diagnostics |

### 7.2 Freshness Score (Client Selection)

```
freshness_score = Î£_{label} Î£_{bin} min(amount_to_use, available) Ã— decay_rate^avg_usage
```

Where:
- `avg_usage = (bin_min + bin_max) / 2.0`
- Bins are exponential: [0], [1], [2-3], [4-7], [8-15], ...
- `decay_rate` is configurable (default: 0.95)

| | |
|---|---|
| **Higher = better** | More fresh (less frequently used) data |
| **Purpose** | USFL selector Phase 3 â€” prioritizes clients with underused data |
| **Location** | `usfl_selector.py:247-289` |
| **Saved?** | Logged to USFLLogger file |

### 7.3 Data Balancing Metrics

| Metric | Description | Location |
|--------|-------------|----------|
| `added_count` | Samples added via replication/target | `usfl_stage_organizer.py:586-697` |
| `removed_count` | Samples removed via trimming/target | Same |
| `augmented_dataset_sizes` | Per-client per-label adjusted data sizes | `usfl_stage_organizer.py:746` |

Saved as `CLIENT_DATA_USAGE_PER_ROUND` event in the SFL result JSON.

### 7.4 Gradient Shuffle Metrics (Adaptive Alpha)

For the `average_adaptive_alpha` gradient shuffle strategy:

```
cos_sim_i = (grad_i Â· mean_grad) / (â€–grad_iâ€– Ã— â€–mean_gradâ€–)
alpha_i   = sigmoid(Î² Ã— cos_sim_i)
shuffled_i = alpha_i Ã— grad_i + (1 - alpha_i) Ã— mean_grad
```

| | |
|---|---|
| **Purpose** | Per-sample adaptive mixing weight based on gradient alignment |
| **Î²** | Configurable `adaptive_alpha_beta` parameter |
| **Location** | `usfl_stage_organizer.py:1179-1190` |
| **Saved?** | Logged via print (not in result JSON) |

### 7.5 Aggregation Weights (Label-Capped)

USFL uses a label-aware aggregation instead of standard FedAvg:

```
max_weight[l] = n_l / N          (per-label cap from global distribution)
weight_j = Î£_l max_weight[l] Ã— (n_{l,j} / n_l)
weights = normalize(weights)     (so Î£ w_j = 1)
```

| | |
|---|---|
| **Purpose** | Prevents clients with many samples of rare classes from dominating the aggregation |
| **Comparison** | FedAvg uses `w_j = dataset_size_j / total_size` (label-blind) |
| **Location** | `usfl_aggregator.py:55-70` |

---

## 8. MultiSFL-Specific Metrics

### 8.1 FGN (Functional Gradient Norm)

```
FGN_r = mean([-lr_server Ã— grad_norm_sq_per_branch])
```

Where `grad_norm_sq = Î£_param â€–param.gradâ€–Â²` computed per branch.

| | |
|---|---|
| **Purpose** | Drives the sampling proportion scheduler â€” tracks how fast the model is learning |
| **Location** | `multisfl/trainer.py:535-538` |

### 8.2 Sampling Proportion (p_r)

Controls what fraction of training data comes from replay vs. new client data. Updated each round using FGN:

| Mode | Formula |
|------|---------|
| `paper` | `p_{r+1} = p_r Ã— (1 + (FGN_r - FGN_{r-1}) / FGN_{r-1})` |
| `abs_ratio` | `p_{r+1} = p_r Ã— \|FGN_r\| / (\|FGN_{r-1}\| + Îµ)` |
| `one_plus_delta` | `p_{r+1} = p_r Ã— (1 + clip(Î´, -Î´_clip, +Î´_clip))` where `Î´ = (FGN_r - FGN_{r-1}) / (\|FGN_{r-1}\| + Îµ)` |

All clipped to `[p_min, p_max]`.

| | |
|---|---|
| **Location** | `multisfl/scheduler.py:37-79` |
| **Saved?** | Yes â€” `p_r` per round in result JSON |

### 8.3 Score Vector & Knowledge Replay

**Score Vector (per-branch):**
```
sv = Î£_j Î³^{r-j} Ã— L_j / Î£_j Î³^{r-j}
```

Where `L_j` is the label distribution at round j and `Î³` is a decay factor. This is an exponentially-weighted moving average of historical label distributions.

**Replay Prior (per-branch, per-class):**
```
prior[c] = max(0, mean(sv) - sv[c])
```

Identifies underrepresented classes by comparing each class's score to the mean.

**Quota (replay samples requested):**
```
total = round(base_count Ã— p_r)
q[c]  = round(total Ã— prior[c] / Î£ prior)
```

| | |
|---|---|
| **Purpose** | Plans replay of past client data to fill knowledge gaps in each branch |
| **Location** | `multisfl/replay.py:23-81` |
| **Saved?** | `requested`, `collected`, `trials` per round in result JSON |

### 8.4 Server Training Metrics (per-branch)

| Metric | Formula | Purpose |
|--------|---------|---------|
| `grad_norm_sq` | `Î£_param â€–param.gradâ€–Â²` | Server gradient magnitude |
| `grad_f_main_norm` | `â€–âˆ‚L/âˆ‚activationsâ€–` | Split-point gradient magnitude |
| `server_param_update_norm` | `âˆš(Î£_param â€–p_after - p_beforeâ€–Â²)` | How much server params changed per step |

Aggregated to per-round means:
- `mean_grad_f_main_norm` â€” averaged over branches and steps
- `mean_server_update_norm` â€” same

| **Location** | `multisfl/servers.py:193-215` |
|---|---|

### 8.5 Client Training Metrics

| Metric | Formula | Purpose |
|--------|---------|---------|
| `param_update_norm` | `âˆš(Î£_param â€–p_after - p_beforeâ€–Â²)` | Client model change per step |
| `label_dist` | `count[c] / total_count` | Normalized label distribution of batch |

Aggregated: `mean_client_update_norm` per round.

### 8.6 Soft-Pull Blending

After each round, branch models are blended toward the master:

```
branch_new = (branch + Î± Ã— master) / (1 + Î±)
```

Where `Î± = alpha_master_pull`. This is applied independently to both client-side and server-side branch models.

| | |
|---|---|
| **Purpose** | Prevents branch divergence while allowing specialization |
| **Location** | `multisfl/utils.py:54-64`, `servers.py:66-73, 230-237` |

---

## 9. Configuration Reference

All metric-related configuration lives in `experiment_configs/common.json`:

| Key | Default | Description |
|-----|---------|-------------|
| `enable_g_measurement` | `false` | Enable G measurement system |
| `g_measurement_mode` | `"single"` | Gradient collection mode: `"single"`, `"k_batch"`, `"accumulated"` |
| `g_measurement_k` | `5` | Number of batches for k_batch mode |
| `use_variance_g` | `false` | Enable weighted-variance G decomposition |
| `g_measure_frequency` | `10` | Measure G every N rounds (modulo-based: `(round+1) % N == 0`) |
| `g_oracle_mode` | `"global"` | Oracle mode: `"global"` (single oracle) or `"individual"` (per-model) |
| `enable_drift_measurement` | `true` | Enable drift tracking + A_cos + M_norm |
| `drift_sample_interval` | `1` | Accumulate drift every N local steps (1 = every step) |

### Oracle Mode Mapping

| Common Config | SFL Adapter | GAS Adapter | MultiSFL Adapter |
|---------------|-------------|-------------|------------------|
| `"global"` | (direct) | `"strict"` | `"master"` |
| `"individual"` | (direct) | `"realistic"` | `"branch"` |

---

## 10. Result JSON Schema

### SFL Framework

```jsonc
{
  "config": { /* all Config fields */ },
  "metric": {
    "0": [
      {"timestamp": "...", "event": "PRE_ROUND_START", "params": {}},
      {"event": "CLIENTS_SELECTED", "params": {"client_ids": [...]}},
      {"event": "MODEL_EVALUATED", "params": {"accuracy": 0.85}},
      {"event": "G_MEASUREMENT", "params": {/* RoundGMeasurement */}},
      {"event": "DRIFT_MEASUREMENT", "params": {/* DriftMetrics */}},
      {"event": "CLIENT_DATA_USAGE_PER_ROUND", "params": {/* USFL only */}}
    ]
  },
  "g_measurements": [
    {"round": 9, "params": {
      "server": {"G": ..., "G_rel": ..., "D_cosine": ...},
      "client_G_mean": ..., "client_G_max": ..., "client_D_mean": ...,
      "per_client": {/* client_id -> {G, G_rel, D_cosine} */},
      "split_layer": {"G": ..., "G_rel": ..., "D_cosine": ...},
      "variance_client_g": ..., "variance_client_g_rel": ...,
      "variance_server_g": ..., "variance_server_g_rel": ...
    }}
  ],
  "drift_history": {
    "G_drift": [...],                      // per-round (legacy alias)
    "G_drift_client": [...],
    "G_drift_client_stepweighted": [...],
    "G_end": [...],                        // legacy alias
    "G_end_client": [...],
    "G_end_client_weighted": [...],
    "G_drift_norm": [...],                 // legacy alias
    "G_drift_norm_client": [...],
    "delta_global_norm_sq": [...],         // legacy alias
    "delta_client_norm_sq": [...],
    "D_dir_client_weighted": [...],
    "D_rel_client_weighted": [...],
    "G_drift_server": [...],
    "G_end_server": [...],
    "G_drift_norm_server": [...],
    "delta_server_norm_sq": [...],
    "G_drift_total": [...],
    "G_end_total": [...],
    "A_cos": [...],
    "M_norm": [...],
    "n_valid_alignment": [...],
    "per_round": [/* full DriftMetrics per round */]
  }
}
```

### GAS Framework

```jsonc
{
  "config": { /* experiment settings */ },
  "accuracy": [...],                       // per-round
  "v_value": [...],                        // per-round (if V_Test=True)
  "time_record": [...],                    // per-round (if WRTT=True)
  "g_history": {                           // NOTE: different key from SFL!
    "client_g": [...],
    "client_g_rel": [...],
    "client_d": [...],
    "server_g": [...],
    "server_g_rel": [...],
    "server_d": [...],
    "split_g": [...],
    "variance_client_g": [...],
    "variance_client_g_rel": [...],
    "variance_server_g": [...],
    "variance_server_g_rel": [...],
    "per_client_g": [{/* client_id -> {G, G_rel, D} */}],
    "per_server_g": [{/* {G, G_rel, D} */}]
  },
  "drift_history": { /* same structure as SFL */ }
}
```

### MultiSFL Framework

```jsonc
{
  "config": { /* CLI args */ },
  "rounds": [
    {
      "round": 0,
      "accuracy": 0.10,
      "p_r": 0.5,
      "fgn_r": -0.001,
      "requested": 100,
      "collected": 95,
      "trials": 120,
      "mean_grad_f_main_norm": 0.05,
      "mean_client_update_norm": 0.02,
      "mean_server_update_norm": 0.03,
      // If G measurement enabled:
      "client_g": ..., "client_g_rel": ...,
      "server_g": ..., "server_g_rel": ...,
      "per_client_g": {...}, "per_branch_server_g": {...}
    }
  ],
  "summary": {
    "final_accuracy": 0.85,
    "best_accuracy": 0.87,
    "total_requested": 10000,
    "total_collected": 9500
  },
  "g_measurements": [/* detailed G data per diagnostic round */],
  "drift_history": {
    // Same as SFL/GAS but with additional:
    "A_cos_client": [...],
    "M_norm_client": [...],
    "A_cos_server": [...],       // unique to MultiSFL
    "M_norm_server": [...]       // unique to MultiSFL
  }
}
```

### Known Inconsistencies

| Issue | Details |
|-------|---------|
| G measurement key name | GAS uses `g_history`, SFL/MultiSFL use `g_measurements` |
| Accuracy location | SFL: inside `metric` events, GAS: top-level `accuracy` array, MultiSFL: inside `rounds` array |
| A_cos scope | SFL/GAS: single `A_cos`, MultiSFL: separate `A_cos_client` + `A_cos_server` |
